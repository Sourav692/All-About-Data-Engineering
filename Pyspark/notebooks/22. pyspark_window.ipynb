{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|James        |Sales     |3000  |1         |\n",
      "|James        |Sales     |3000  |2         |\n",
      "|Robert       |Sales     |4100  |3         |\n",
      "|Saif         |Sales     |4100  |4         |\n",
      "|Michael      |Sales     |4600  |5         |\n",
      "|Maria        |Finance   |3000  |1         |\n",
      "|Scott        |Finance   |3300  |2         |\n",
      "|Jen          |Finance   |3900  |3         |\n",
      "|Kumar        |Marketing |2000  |1         |\n",
      "|Jeff         |Marketing |3000  |2         |\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 rank Window Function\n",
    "rank() window function is used to provide a rank to the result within a window partition. This function leaves gaps in rank when there are ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|   1|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|       Robert|     Sales|  4100|   3|\n",
      "|         Saif|     Sales|  4100|   3|\n",
      "|      Michael|     Sales|  4600|   5|\n",
      "|        Maria|   Finance|  3000|   1|\n",
      "|        Scott|   Finance|  3300|   2|\n",
      "|          Jen|   Finance|  3900|   3|\n",
      "|        Kumar| Marketing|  2000|   1|\n",
      "|         Jeff| Marketing|  3000|   2|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"rank\",rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 dense_rank Window Function\n",
    "dense_rank() window function is used to get the result with rank of rows within a window partition without any gaps. This is similar to rank() function difference being rank function leaves gaps in rank when there are ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|dense_rank|\n",
      "+-------------+----------+------+----------+\n",
      "|        James|     Sales|  3000|         1|\n",
      "|        James|     Sales|  3000|         1|\n",
      "|       Robert|     Sales|  4100|         2|\n",
      "|         Saif|     Sales|  4100|         2|\n",
      "|      Michael|     Sales|  4600|         3|\n",
      "|        Maria|   Finance|  3000|         1|\n",
      "|        Scott|   Finance|  3300|         2|\n",
      "|          Jen|   Finance|  3900|         3|\n",
      "|        Kumar| Marketing|  2000|         1|\n",
      "|         Jeff| Marketing|  3000|         2|\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 percent_rank Window Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------+\n",
      "|employee_name|department|salary|percent_rank|\n",
      "+-------------+----------+------+------------+\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|       Robert|     Sales|  4100|         0.5|\n",
      "|         Saif|     Sales|  4100|         0.5|\n",
      "|      Michael|     Sales|  4600|         1.0|\n",
      "|        Maria|   Finance|  3000|         0.0|\n",
      "|        Scott|   Finance|  3300|         0.5|\n",
      "|          Jen|   Finance|  3900|         1.0|\n",
      "|        Kumar| Marketing|  2000|         0.0|\n",
      "|         Jeff| Marketing|  3000|         1.0|\n",
      "+-------------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5 ntile Window Function\n",
    "ntile() window function returns the relative rank of result rows within a window partition. In below example we have used 2 as an argument to ntile hence it returns ranking between 2 values (1 and 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----+\n",
      "|employee_name|department|salary|ntile|\n",
      "+-------------+----------+------+-----+\n",
      "|        James|     Sales|  3000|    1|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|       Robert|     Sales|  4100|    1|\n",
      "|         Saif|     Sales|  4100|    2|\n",
      "|      Michael|     Sales|  4600|    2|\n",
      "|        Maria|   Finance|  3000|    1|\n",
      "|        Scott|   Finance|  3300|    1|\n",
      "|          Jen|   Finance|  3900|    2|\n",
      "|        Kumar| Marketing|  2000|    1|\n",
      "|         Jeff| Marketing|  3000|    2|\n",
      "+-------------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import ntile\n",
    "df.withColumn(\"ntile\",ntile(2).over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 cume_dist Window Function\n",
    "cume_dist() window function is used to get the cumulative distribution of values within a window partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------------+\n",
      "|employee_name|department|salary|         cume_dist|\n",
      "+-------------+----------+------+------------------+\n",
      "|        James|     Sales|  3000|               0.4|\n",
      "|        James|     Sales|  3000|               0.4|\n",
      "|       Robert|     Sales|  4100|               0.8|\n",
      "|         Saif|     Sales|  4100|               0.8|\n",
      "|      Michael|     Sales|  4600|               1.0|\n",
      "|        Maria|   Finance|  3000|0.3333333333333333|\n",
      "|        Scott|   Finance|  3300|0.6666666666666666|\n",
      "|          Jen|   Finance|  3900|               1.0|\n",
      "|        Kumar| Marketing|  2000|               0.5|\n",
      "|         Jeff| Marketing|  3000|               1.0|\n",
      "+-------------+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import cume_dist    \n",
    "df.withColumn(\"cume_dist\",cume_dist().over(windowSpec)) \\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 lag Window Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary| lag|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|null|\n",
      "|        James|     Sales|  3000|null|\n",
      "|       Robert|     Sales|  4100|3000|\n",
      "|         Saif|     Sales|  4100|3000|\n",
      "|      Michael|     Sales|  4600|4100|\n",
      "|        Maria|   Finance|  3000|null|\n",
      "|        Scott|   Finance|  3300|null|\n",
      "|          Jen|   Finance|  3900|3000|\n",
      "|        Kumar| Marketing|  2000|null|\n",
      "|         Jeff| Marketing|  3000|null|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag    \n",
    "df.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)) \\\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 lead Window Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|lead|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|4100|\n",
      "|        James|     Sales|  3000|4100|\n",
      "|       Robert|     Sales|  4100|4600|\n",
      "|         Saif|     Sales|  4100|null|\n",
      "|      Michael|     Sales|  4600|null|\n",
      "|        Maria|   Finance|  3000|3900|\n",
      "|        Scott|   Finance|  3300|null|\n",
      "|          Jen|   Finance|  3900|null|\n",
      "|        Kumar| Marketing|  2000|null|\n",
      "|         Jeff| Marketing|  3000|null|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead    \n",
    "df.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. PySpark Window Aggregate Functions\n",
    "In this section, I will explain how to calculate sum, min, max for each department using PySpark SQL Aggregate window functions and WindowSpec. When working with Aggregate functions, we donâ€™t need to use order by clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+----+----+---+\n",
      "|department|   avg|  sum| min| max|row|\n",
      "+----------+------+-----+----+----+---+\n",
      "|     Sales|3760.0|18800|3000|4600|  1|\n",
      "|     Sales|3760.0|18800|3000|4600|  2|\n",
      "|     Sales|3760.0|18800|3000|4600|  3|\n",
      "|     Sales|3760.0|18800|3000|4600|  4|\n",
      "|     Sales|3760.0|18800|3000|4600|  5|\n",
      "|   Finance|3400.0|10200|3000|3900|  1|\n",
      "|   Finance|3400.0|10200|3000|3900|  2|\n",
      "|   Finance|3400.0|10200|3000|3900|  3|\n",
      "| Marketing|2500.0| 5000|2000|3000|  1|\n",
      "| Marketing|2500.0| 5000|2000|3000|  2|\n",
      "+----------+------+-----+----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number \n",
    "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .select(\"department\",\"avg\",\"sum\",\"min\",\"max\",\"row\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+----+----+\n",
      "|department|   avg|  sum| min| max|\n",
      "+----------+------+-----+----+----+\n",
      "|     Sales|3760.0|18800|3000|4600|\n",
      "|   Finance|3400.0|10200|3000|3900|\n",
      "| Marketing|2500.0| 5000|2000|3000|\n",
      "+----------+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To get the first row for every department\n",
    "\n",
    "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0012cd877e37c553ce082c8a53dbb8150686811a71a997c633aba52086b562f8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pyspark3': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
