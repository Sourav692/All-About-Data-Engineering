{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark provides map(), mapPartitions() to loop/iterate through rows in RDD/DataFrame to perform the complex transformations, and these two returns the same number of records as in the original DataFrame but the number of columns could be different (after add/update).\n",
    "\n",
    "PySpark also provides foreach() & foreachPartitions() actions to loop/iterate through each Row in a DataFrame but these two returns nothing, In this article, I will explain how to use these methods to get DataFrame column values and process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+----+\n",
      "|firstname|lastname|gender|salary|flag|\n",
      "+---------+--------+------+------+----+\n",
      "|    James|   Smith|     M|    30|true|\n",
      "|     Anna|    Rose|     F|    41|true|\n",
      "|   Robert|Williams|     M|    62|true|\n",
      "+---------+--------+------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "data = [('James','Smith','M',30,True),('Anna','Rose','F',41,True),\n",
    "  ('Robert','Williams','M',62,True), \n",
    "]\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\",\"flag\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|header                               |\n",
      "+-------------------------------------+\n",
      "|Employee_Data                        |\n",
      "|firstname,lastname,gender,salary,flag|\n",
      "|James,Smith,M,30,true                |\n",
      "|Anna,Rose,F,41,true                  |\n",
      "|Robert,Williams,M,62,true            |\n",
      "|Employee_Data_Trailer                |\n",
      "+-------------------------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'option'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SOURAV~1\\AppData\\Local\\Temp/ipykernel_15492/2256846305.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mdf_final\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_final\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mdf_final\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"gzip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./path1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\envs\\pyspark3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1641\u001b[0m         \"\"\"\n\u001b[0;32m   1642\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1643\u001b[1;33m             raise AttributeError(\n\u001b[0m\u001b[0;32m   1644\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0;32m   1645\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'option'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "#col_list = df.columns\n",
    "df_concat = df.withColumn(\"concatenated_cols\",concat_ws(\",\",*df.columns)).select(\"concatenated_cols\")\n",
    "\n",
    "df_header = spark.sql(\"SELECT 'Employee_Data' as header\")\n",
    "\n",
    "header_string = \",\".join(df.columns)\n",
    "df_columns = spark.sql(f\"select '{header_string}'\")\n",
    "\n",
    "df_trailer = spark.sql(\"SELECT 'Employee_Data_Trailer' as trailer\")\n",
    "\n",
    "df_final = df_header.union(df_columns).union(df_concat).union(df_trailer)\n",
    "\n",
    "#df_final.show(df_final.count(),False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.repartition(1).write.mode(\"overwrite\").csv(\".\\path2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|firstname,lastname,gender,salary,flag|\n",
      "+-------------------------------------+\n",
      "|                 firstname,lastnam...|\n",
      "+-------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'firstname,lastname,gender,salary,flag'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "header_string = \",\".join(df.columns)\n",
    "df_columns = spark.sql(f\"select '{header_string}'\")\n",
    "df_columns.show()\n",
    "header_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select '\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+----------+\n",
      "|           name|gender|new_salary|\n",
      "+---------------+------+----------+\n",
      "|    James,Smith|     M|        60|\n",
      "|      Anna,Rose|     F|        82|\n",
      "|Robert,Williams|     M|       124|\n",
      "+---------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Mostly for simple computations, instead of iterating through using map() and foreach(), \n",
    "# you should use either DataFrame select() or DataFrame withColumn() in conjunction with PySpark SQL functions.\n",
    "\n",
    "from pyspark.sql.functions import concat_ws,col,lit\n",
    "df.select(concat_ws(\",\",df.firstname,df.lastname).alias(\"name\"), \\\n",
    "          df.gender,lit(df.salary*2).alias(\"new_salary\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using map() to Loop Through Rows in DataFrame\n",
    "\n",
    "PySpark map() Transformation is used to loop/iterate through the PySpark DataFrame/RDD by applying the transformation function (lambda) on every element (Rows and Columns) of RDD/DataFrame. PySpark doesn’t have a map() in DataFrame instead it’s in RDD hence we need to convert DataFrame to RDD first and then use the map(). It returns an RDD and you should Convert RDD to PySpark DataFrame if needed.\n",
    "\n",
    "If you have a heavy initialization use PySpark mapPartitions() transformation instead of map(), as with mapPartitions() heavy initialization executes only once for each partition instead of every record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+----------+\n",
      "|           name|gender|new_salary|\n",
      "+---------------+------+----------+\n",
      "|    James,Smith|     M|        60|\n",
      "|      Anna,Rose|     F|        82|\n",
      "|Robert,Williams|     M|       124|\n",
      "+---------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 =df.rdd.map(lambda x: (x[0]+\",\"+x[1],x[2],x[3]*2)).toDF([\"name\",\"gender\",\"new_salary\"])\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('James,Smith', 'M', 60),\n",
       " ('Anna,Rose', 'F', 82),\n",
       " ('Robert,Williams', 'M', 124)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2=df.rdd.map(lambda x: \n",
    "    (x[\"firstname\"]+\",\"+x[\"lastname\"],x[\"gender\"],x[\"salary\"]*2)\n",
    "    ) \n",
    "\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referring Column Names\n",
    "rdd2=df.rdd.map(lambda x: \n",
    "    (x.firstname+\",\"+x.lastname,x.gender,x.salary*2)\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('James,Smith', 'm', 60),\n",
       " ('Anna,Rose', 'f', 82),\n",
       " ('Robert,Williams', 'm', 124)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By Calling function\n",
    "def func1(x):\n",
    "    # firstName=x.firstname\n",
    "    # lastname=x.lastName\n",
    "    name=x.firstname+\",\"+x.lastname\n",
    "    gender=x.gender.lower()\n",
    "    salary=x.salary*2\n",
    "    return (name,gender,salary)\n",
    "\n",
    "rdd2=df.rdd.map(lambda x: func1(x))\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('James,Smith', 'm', 60)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using foreach() to Loop Through Rows in DataFrame\n",
    "\n",
    "Similar to map(), foreach() also applied to every row of DataFrame, the difference being foreach() is an action and it returns nothing. Below are some examples to iterate through DataFrame using for each.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foreach example\n",
    "def f(x): print(x)\n",
    "df.foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example\n",
    "df.foreach(lambda x: \n",
    "    print(\"Data ==>\"+x[\"firstname\"]+\",\"+x[\"lastname\"]+\",\"+x[\"gender\"]+\",\"+str(x[\"salary\"]*2))\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James,Smith\n",
      "Anna,Rose\n",
      "Robert,Williams\n",
      "James,Smith\n",
      "Anna,Rose\n",
      "Robert,Williams\n"
     ]
    }
   ],
   "source": [
    "# Collect the data to Python List\n",
    "dataCollect = df.collect()\n",
    "for row in dataCollect:\n",
    "    print(row['firstname'] + \",\" +row['lastname'])\n",
    "\n",
    "#Using toLocalIterator()\n",
    "dataCollect=df.rdd.toLocalIterator()\n",
    "for row in dataCollect:\n",
    "    print(row['firstname'] + \",\" +row['lastname'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0012cd877e37c553ce082c8a53dbb8150686811a71a997c633aba52086b562f8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pyspark3': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
