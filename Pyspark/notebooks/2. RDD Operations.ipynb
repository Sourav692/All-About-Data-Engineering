{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b65640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58a5e582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n",
    "my_list = [11, 12, 17, 14, 10, 13]\n",
    "rdd = spark.sparkContext.parallelize(my_list)\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3898da5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03f668af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 4, 4, 2], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07764b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created numbers rdd with 4 partitions\n"
     ]
    }
   ],
   "source": [
    "print(\"created numbers rdd with %d partitions\" % rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75f36573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark',\n",
       " 'Unified',\n",
       " 'Computation',\n",
       " 'Engine',\n",
       " ':',\n",
       " 'Big',\n",
       " 'Data',\n",
       " 'Processing',\n",
       " 'Made',\n",
       " 'Simple']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myCollection = \"Spark Unified Computation Engine : Big Data Processing Made Simple\".split(\" \")\n",
    "myCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a13a080e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created words rdd with 2 partitions\n"
     ]
    }
   ],
   "source": [
    "words = sc.parallelize(myCollection, 2)\n",
    "print(\"created words rdd with %d partitions\" % words.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39582502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def startsWithS(individual):\n",
    "  return individual.startswith(\"S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e713f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark', 'Simple']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply filter transformation\n",
    "filteredWords = words.filter(lambda word: startsWithS(word))\n",
    "filteredWords.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "647faa2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 'S', True),\n",
       " ('Unified', 'U', False),\n",
       " ('Computation', 'C', False),\n",
       " ('Engine', 'E', False),\n",
       " (':', ':', False),\n",
       " ('Big', 'B', False),\n",
       " ('Data', 'D', False),\n",
       " ('Processing', 'P', False),\n",
       " ('Made', 'M', False),\n",
       " ('Simple', 'S', True)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply map transformation\n",
    "\n",
    "mapTransformation = words.map(lambda word: (word, word[0], word.startswith(\"S\")))\n",
    "display(mapTransformation.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8ff71a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 'S', True), ('Simple', 'S', True)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filteredWords = mapTransformation.filter(lambda record: record[2])\n",
    "display(filteredWords.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4277ae54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Computation',\n",
       " 'Processing',\n",
       " 'Unified',\n",
       " 'Engine',\n",
       " 'Simple',\n",
       " 'Spark',\n",
       " 'Data',\n",
       " 'Made',\n",
       " 'Big',\n",
       " ':']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform sorting\n",
    "words.sortBy(lambda word: len(word) * -1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31acbbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Computation', 'Processing']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "words.sortBy(lambda word: len(word) * -1).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e011971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of numbers from 1 to 20 is: 210\n"
     ]
    }
   ],
   "source": [
    "# Perform reduce operation\n",
    "# Sum of all numbers from 1 to 20\n",
    "# Sn = n*(n1 + nl) / 2 (Sum of A.P.)\n",
    "\n",
    "print(\"Sum of numbers from 1 to 20 is: %d\" % sc.parallelize(range(1, 21)).reduce(lambda x, y: x + y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "257603fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Computation'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# Get the word with the highest length\n",
    "\n",
    "def wordLengthReducer(leftWord, rightWord):\n",
    "  if len(leftWord) > len(rightWord):\n",
    "    return leftWord\n",
    "  else:\n",
    "    return rightWord\n",
    "\n",
    "words.reduce(wordLengthReducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03ef2b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the processing...\n",
      "(5, 'Spark')\n",
      "(7, 'Unified')\n",
      "(11, 'Computation')\n",
      "(6, 'Engine')\n",
      "(1, ':')\n",
      "Finishing the processing!\n",
      "Starting the processing...\n",
      "(3, 'Big')\n",
      "(4, 'Data')\n",
      "(10, 'Processing')\n",
      "(4, 'Made')\n",
      "(6, 'Simple')\n",
      "Finishing the processing!\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# Apply a function for every partition\n",
    "\n",
    "def processingFunction(iter):\n",
    "  result = []\n",
    "  result.append(\"Starting the processing...\")\n",
    "  result += [(len(i), i) for i in iter]\n",
    "  result.append(\"Finishing the processing!\")\n",
    "  return result \n",
    "\n",
    "transformedWords = words.mapPartitions(processingFunction)\n",
    "for i in transformedWords.collect():\n",
    "  print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27523470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark\n",
      "Unified\n",
      "Computation\n",
      "Engine\n",
      ":\n",
      "Big\n",
      "Data\n",
      "Processing\n",
      "Made\n",
      "Simple\n"
     ]
    }
   ],
   "source": [
    "for i in words.collect():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d371f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['partition: 0 => Spark',\n",
       " 'partition: 0 => Unified',\n",
       " 'partition: 0 => Computation',\n",
       " 'partition: 0 => Engine',\n",
       " 'partition: 0 => :',\n",
       " 'partition: 1 => Big',\n",
       " 'partition: 1 => Data',\n",
       " 'partition: 1 => Processing',\n",
       " 'partition: 1 => Made',\n",
       " 'partition: 1 => Simple']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply a function for every partition, using the information about the index of the partition\n",
    "\n",
    "def indexedFunc(partitionIndex, withinPartitionIterator):\n",
    "  return [\"partition: {} => {}\".format(partitionIndex, x) for x in withinPartitionIterator]\n",
    "\n",
    "words.mapPartitionsWithIndex(indexedFunc).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1adebed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire rdd: [100, 2, 3, 3, 410, 3, 3, 3, 4, 104, 2]\n",
      "Number of elements in the rdd: 11\n",
      "Distinct elements in the rdd: [100, 2, 3, 410, 4, 104]\n",
      "First element in the rdd: 100\n",
      "Random two elements in the rdd: [100, 2]\n",
      "Frequency of each element in the rdd: defaultdict(<class 'int'>, {100: 1, 2: 2, 3: 5, 410: 1, 4: 1, 104: 1})\n",
      "Maximum element in the rdd: 410\n",
      "Minimum element in the rdd: 2\n",
      "Bottom 2 elements in the rdd: [2, 2]\n",
      "Top 2 elements in the rdd: [410, 104]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([100, 2, 3, 3, 410, 3, 3, 3, 4, 104, 2])\n",
    "print(\"Entire rdd: %s\" % rdd.collect())\n",
    "print(\"Number of elements in the rdd: %d\" % rdd.count())\n",
    "print(\"Distinct elements in the rdd: %s\" % rdd.distinct().collect())\n",
    "print(\"First element in the rdd: %d\" % rdd.first())\n",
    "print(\"Random two elements in the rdd: %s\" % rdd.take(2))\n",
    "print(\"Frequency of each element in the rdd: %s\" % rdd.countByValue())\n",
    "print(\"Maximum element in the rdd: %s\" % rdd.max())\n",
    "print(\"Minimum element in the rdd: %s\" % rdd.min())\n",
    "print(\"Bottom 2 elements in the rdd: %s\" % rdd.takeOrdered(2))\n",
    "print(\"Top 2 elements in the rdd: %s\" % rdd.takeOrdered(2, key = lambda x: -x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c926c980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
