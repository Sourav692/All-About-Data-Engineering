{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PySpark Row class is available by importing pyspark.sql.Row which is represented as a record/row in DataFrame, one can create a Row object by using named arguments, or create a custom Row like class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Points of Row Class:\n",
    "\n",
    "    Earlier to Spark 3.0, when used Row class with named arguments, the fields are sorted by name.\n",
    "    Since 3.0, Rows created from named arguments are not sorted alphabetically instead they will be ordered in the position entered.\n",
    "    To enable sorting by names, set the environment variable PYSPARK_ROW_FIELD_SORTING_ENABLED to true.\n",
    "    Row class provides a way to create a struct-type column as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Name is Sourav and I am a Data Engineer\n"
     ]
    }
   ],
   "source": [
    "## 1. Create a Row Object\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "row = Row(\"Sourav\",\"Data Engineer\")\n",
    "\n",
    "print(f\"My Name is {row[0]} and I am a {row[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Name is Sourav and I am a Data Engineer\n"
     ]
    }
   ],
   "source": [
    "## Alternatively you can also write with named arguments. Benefits with the named argument is you can access with field name row.name.\n",
    "\n",
    "\n",
    "row1 = Row(name=\"Sourav\",skill=\"Data Engineer\")\n",
    "\n",
    "\n",
    "print(f\"My Name is {row1.name} and I am a {row1.skill}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Custom Class from Row\n",
    "\n",
    "We can also create a Row like class, for example “Person” and use it similar to Row object. This would be helpful when you wanted to create real time object and refer it’s properties. On below example, we have created a Person class and used similar to Row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James,Alice\n"
     ]
    }
   ],
   "source": [
    "Person = Row(\"name\", \"age\")\n",
    "p1=Person(\"James\", 40)\n",
    "p2=Person(\"Alice\", 35)\n",
    "print(p1.name +\",\"+p2.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using Row class on PySpark RDD\n",
    "\n",
    "We can use Row class on PySpark RDD. When you use Row to create an RDD, after collecting the data you will get the result back in Row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(name='James,,Smith', lang=['Java', 'Scala', 'C++'], state='CA'), Row(name='Michael,Rose,', lang=['Spark', 'Java', 'C++'], state='NJ'), Row(name='Robert,,Williams', lang=['CSharp', 'VB'], state='NV')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "data1 = [Row(name=\"James,,Smith\",lang=[\"Java\",\"Scala\",\"C++\"],state=\"CA\"), \n",
    "    Row(name=\"Michael,Rose,\",lang=[\"Spark\",\"Java\",\"C++\"],state=\"NJ\"),\n",
    "    Row(name=\"Robert,,Williams\",lang=[\"CSharp\",\"VB\"],state=\"NV\")]\n",
    "rdd=spark.sparkContext.parallelize(data1)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James,,Smith,['Java', 'Scala', 'C++']\n",
      "Michael,Rose,,['Spark', 'Java', 'C++']\n",
      "Robert,,Williams,['CSharp', 'VB']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "collData=rdd.collect()\n",
    "for row in collData:\n",
    "    print(row.name + \",\" +str(row.lang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James,,Smith,['Java', 'Scala', 'C++']\n",
      "Michael,Rose,,['Spark', 'Java', 'C++']\n",
      "Robert,,Williams,['CSharp', 'VB']\n"
     ]
    }
   ],
   "source": [
    "## Alternatively, you can also do by creating a Row like class “Person”\n",
    "\n",
    "Person=Row(\"name\",\"lang\",\"state\")\n",
    "data = [Person(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \n",
    "    Person(\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"),\n",
    "    Person(\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "\n",
    "collData=rdd.collect()\n",
    "for row in collData:\n",
    "    print(row.name + \",\" +str(row.lang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James,,Smith,['Java', 'Scala', 'C++']\n",
      "Michael,Rose,,['Spark', 'Java', 'C++']\n",
      "Robert,,Williams,['CSharp', 'VB']\n"
     ]
    }
   ],
   "source": [
    "## Create RDD using row without names arg\n",
    "\n",
    "data = [Row(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \n",
    "    Row(\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"),\n",
    "    Row(\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
    "\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "collData=rdd.collect()\n",
    "\n",
    "for row in collData:\n",
    "    print(row[0]+ \",\" +str(row[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Using Row class on PySpark DataFrame\n",
    "\n",
    "\n",
    "Similarly, Row class also can be used with PySpark DataFrame, By default data in DataFrame represent as Row. To demonstrate, I will use the same data that was created for RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- lang: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+-----+\n",
      "|            name|              lang|state|\n",
      "+----------------+------------------+-----+\n",
      "|    James,,Smith|[Java, Scala, C++]|   CA|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|   NJ|\n",
      "|Robert,,Williams|      [CSharp, VB]|   NV|\n",
      "+----------------+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(data1)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## You can also change the column names by using toDF() function\n",
    "\n",
    "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "df=spark.createDataFrame(data).toDF(*columns)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create Nested Struct Using Row Class\n",
    "The below example provides a way to create a struct type using the Row class. Alternatively, you can also create struct type using By Providing Schema using PySpark StructType & StructFields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- prop: struct (nullable = true)\n",
      " |    |-- hair: string (nullable = true)\n",
      " |    |-- eye: string (nullable = true)\n",
      "\n",
      "+------+--------------+\n",
      "|  name|          prop|\n",
      "+------+--------------+\n",
      "| James| {black, blue}|\n",
      "|Sourav|{black, black}|\n",
      "+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person = Row(\"name\",\"prop\")\n",
    "property = Row(\"hair\",\"eye\")\n",
    "\n",
    "data3 = [ person(\"James\",property(\"black\",\"blue\")),person(\"Sourav\",property(\"black\",\"black\")) ]\n",
    "\n",
    "df1=spark.createDataFrame(data3)\n",
    "df1.printSchema()    \n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0012cd877e37c553ce082c8a53dbb8150686811a71a997c633aba52086b562f8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pyspark3': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
