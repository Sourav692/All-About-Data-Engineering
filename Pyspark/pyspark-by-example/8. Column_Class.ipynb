{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyspark.sql.Column class provides several functions to work with DataFrame to manipulate the Column values, evaluate the boolean expression to filter rows, retrieve a value or part of a value from a DataFrame column, and to work with list, map & struct columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Points:\n",
    "\n",
    "    PySpark Column class represents a single Column in a DataFrame.\n",
    "    It provides functions that are most used to manipulate DataFrame Columns & Rows.\n",
    "    Some of these Column functions evaluate a Boolean expression that can be used with filter() transformation to filter the DataFrame Rows.\n",
    "    Provides functions to get a value from a list column by index, map value by key & index, and finally struct nested column.\n",
    "    PySpark also provides additional functions pyspark.sql.functions that take Column object and return a Column type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create Column Class Object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One of the simplest ways to create a Column class object is by using PySpark lit() SQL function, this takes a literal value and returns a Column object.\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "colObj = lit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name.fname: string (nullable = true)\n",
      " |-- gender: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"James\",23),(\"Ann\",40)]\n",
    "df=spark.createDataFrame(data).toDF(\"name.fname\",\"gender\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|    23|\n",
      "|    40|\n",
      "+------+\n",
      "\n",
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|    23|\n",
      "|    40|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## You can also access the Column from DataFrame by multiple ways.\n",
    "\n",
    "## Using DataFrame object (df)\n",
    "df.select(df.gender).show()\n",
    "df.select(df[\"gender\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|name.fname|\n",
      "+----------+\n",
      "|     James|\n",
      "|       Ann|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Accessing column name with dot (with backticks)\n",
    "df.select(df[\"`name.fname`\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|    23|\n",
      "|    40|\n",
      "+------+\n",
      "\n",
      "+----------+\n",
      "|name.fname|\n",
      "+----------+\n",
      "|     James|\n",
      "|       Ann|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using SQL col() function\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col(\"gender\")).show()\n",
    "#Accessing column name with dot (with backticks)\n",
    "df.select(col(\"`name.fname`\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below example demonstrates accessing struct type columns. Here I have use PySpark Row class to create a struct type. Alternatively you can also create it by using PySpark StructType & StructField classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- prop: struct (nullable = true)\n",
      " |    |-- hair: string (nullable = true)\n",
      " |    |-- eye: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "data=[Row(name=\"James\",prop=Row(hair=\"black\",eye=\"blue\")),\n",
    "      Row(name=\"Ann\",prop=Row(hair=\"grey\",eye=\"black\"))]\n",
    "df=spark.createDataFrame(data)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|prop.hair|\n",
      "+---------+\n",
      "|    black|\n",
      "|     grey|\n",
      "+---------+\n",
      "\n",
      "+-----+\n",
      "| hair|\n",
      "+-----+\n",
      "|black|\n",
      "| grey|\n",
      "+-----+\n",
      "\n",
      "+-----+\n",
      "| hair|\n",
      "+-----+\n",
      "|black|\n",
      "| grey|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.prop.hair).show()\n",
    "df.select(df[\"prop.hair\"]).show()\n",
    "df.select(col(\"prop.hair\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| hair|  eye|\n",
      "+-----+-----+\n",
      "|black| blue|\n",
      "| grey|black|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Access all columns from struct\n",
    "df.select(col(\"prop.*\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PySpark Column Operators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|(col1 + col2)|\n",
      "+-------------+\n",
      "|          102|\n",
      "|          203|\n",
      "|          304|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col1 - col2)|\n",
      "+-------------+\n",
      "|           98|\n",
      "|          197|\n",
      "|          296|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col1 * col2)|\n",
      "+-------------+\n",
      "|          200|\n",
      "|          600|\n",
      "|         1200|\n",
      "+-------------+\n",
      "\n",
      "+-----------------+\n",
      "|    (col1 / col2)|\n",
      "+-----------------+\n",
      "|             50.0|\n",
      "|66.66666666666667|\n",
      "|             75.0|\n",
      "+-----------------+\n",
      "\n",
      "+-------------+\n",
      "|(col1 % col2)|\n",
      "+-------------+\n",
      "|            0|\n",
      "|            2|\n",
      "|            0|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col2 > col3)|\n",
      "+-------------+\n",
      "|         true|\n",
      "|        false|\n",
      "|        false|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col2 < col3)|\n",
      "+-------------+\n",
      "|        false|\n",
      "|         true|\n",
      "|        false|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col2 = col3)|\n",
      "+-------------+\n",
      "|        false|\n",
      "|        false|\n",
      "|         true|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(100,2,1),(200,3,4),(300,4,4)]\n",
    "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")\n",
    "\n",
    "#Arthmetic operations\n",
    "df.select(df.col1 + df.col2).show()\n",
    "df.select(df.col1 - df.col2).show() \n",
    "df.select(df.col1 * df.col2).show()\n",
    "df.select(df.col1 / df.col2).show()\n",
    "df.select(df.col1 % df.col2).show()\n",
    "\n",
    "df.select(df.col2 > df.col3).show()\n",
    "df.select(df.col2 < df.col3).show()\n",
    "df.select(df.col2 == df.col3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. PySpark Column Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- lname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n",
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|     James| Bond|100|  null|\n",
      "|       Ann|Varsa|200|     F|\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "| Tom Brand| null|400|     M|\n",
      "+----------+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"James\",\"Bond\",\"100\",None),\n",
    "      (\"Ann\",\"Varsa\",\"200\",'F'),\n",
    "      (\"Tom Cruise\",\"XXX\",\"400\",''),\n",
    "      (\"Tom Brand\",None,\"400\",'M')] \n",
    "columns=[\"fname\",\"lname\",\"id\",\"gender\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 alias() – Set’s name to Column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|     James|     Bond|\n",
      "|       Ann|    Varsa|\n",
      "|Tom Cruise|      XXX|\n",
      "| Tom Brand|     null|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.fname.alias(\"first_name\"), \\\n",
    "          df.lname.alias(\"last_name\")\n",
    "   ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|      fullName|\n",
      "+--------------+\n",
      "|    James,Bond|\n",
      "|     Ann,Varsa|\n",
      "|Tom Cruise,XXX|\n",
      "|          null|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df.select(expr(\" fname ||','|| lname\").alias(\"fullName\") \\\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 asc() & desc() – Sort the DataFrame columns by Ascending or Descending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|       Ann|Varsa|200|     F|\n",
      "|     James| Bond|100|  null|\n",
      "| Tom Brand| null|400|     M|\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "+----------+-----+---+------+\n",
      "\n",
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "| Tom Brand| null|400|     M|\n",
      "|     James| Bond|100|  null|\n",
      "|       Ann|Varsa|200|     F|\n",
      "+----------+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#asc, desc to sort ascending and descending order repsectively.\n",
    "df.sort(df.fname.asc()).show()\n",
    "df.sort(df.fname.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 4.3 cast() & astype() – Used to convert the data Type.\n",
    "\n",
    "df.select(df.fname,df.id.cast(\"int\")).printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+------+\n",
      "|fname|lname| id|gender|\n",
      "+-----+-----+---+------+\n",
      "|James| Bond|100|  null|\n",
      "|  Ann|Varsa|200|     F|\n",
      "+-----+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.4 between() – Returns a Boolean expression when a column values in between lower and upper bound.\n",
    "\n",
    "df.filter(df.id.between(100,300)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "+----------+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.5 contains() – Checks if a DataFrame column value contains a a value specified in this function.\n",
    "\n",
    "df.filter(df.fname.contains(\"Cruise\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "| Tom Brand| null|400|     M|\n",
      "+----------+-----+---+------+\n",
      "\n",
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "+----------+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.6 startswith() & endswith() – Checks if the value of the DataFrame Column starts and ends with a String respectively.\n",
    "\n",
    "df.filter(df.fname.startswith(\"T\")).show()\n",
    "df.filter(df.fname.endswith(\"Cruise\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+---+------+\n",
      "|    fname|lname| id|gender|\n",
      "+---------+-----+---+------+\n",
      "|Tom Brand| null|400|     M|\n",
      "+---------+-----+---+------+\n",
      "\n",
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|     James| Bond|100|  null|\n",
      "|       Ann|Varsa|200|     F|\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "+----------+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.8 isNull & isNotNull() – Checks if the DataFrame column has NULL or non NULL values.\n",
    "\n",
    "df.filter(df.lname.isNull()).show()\n",
    "df.filter(df.lname.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "|fname|lname| id|\n",
      "+-----+-----+---+\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.9 like() & rlike() – Similar to SQL LIKE expression\n",
    "df.select(df.fname,df.lname,df.id).filter(df.fname.like(\"%T\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|substr|\n",
      "+------+\n",
      "|    Ja|\n",
      "|    An|\n",
      "|    To|\n",
      "|    To|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.10 substr() – Returns a Column after getting sub string from the Column\n",
    "df.select(df.fname.substr(1,2).alias(\"substr\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.11 when() & otherwise() – It is similar to SQL Case When, executes sequence of expressions until it matches the condition and returns a value when match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+\n",
      "|     fname|lname|new_gender|\n",
      "+----------+-----+----------+\n",
      "|     James| Bond|      null|\n",
      "|       Ann|Varsa|    Female|\n",
      "|Tom Cruise|  XXX|          |\n",
      "| Tom Brand| null|      Male|\n",
      "+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df.select(df.fname,df.lname,when(df.gender==\"M\",\"Male\") \\\n",
    "              .when(df.gender==\"F\",\"Female\") \\\n",
    "              .when(df.gender==None ,\"\") \\\n",
    "              .otherwise(df.gender).alias(\"new_gender\") \\\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "|fname|lname| id|\n",
      "+-----+-----+---+\n",
      "|James| Bond|100|\n",
      "|  Ann|Varsa|200|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 4.12 isin() – Check if value presents in a List.\n",
    "#isin\n",
    "li=[\"100\",\"200\"]\n",
    "df.select(df.fname,df.lname,df.id) \\\n",
    "  .filter(df.id.isin(li)) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- fname: string (nullable = true)\n",
      " |    |-- lname: string (nullable = true)\n",
      " |-- languages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 4.13 getField() – To get the value by key from MapType column and by stuct child name from StructType column\n",
    "\n",
    "#Create DataFrame with struct, array & map\n",
    "from pyspark.sql.types import StructType,StructField,StringType,ArrayType,MapType\n",
    "data=[((\"James\",\"Bond\"),[\"Java\",\"C#\"],{'hair':'black','eye':'brown'}),\n",
    "      ((\"Ann\",\"Varsa\"),[\".NET\",\"Python\"],{'hair':'brown','eye':'black'}),\n",
    "      ((\"Tom Cruise\",\"\"),[\"Python\",\"Scala\"],{'hair':'red','eye':'grey'}),\n",
    "      ((\"Tom Brand\",None),[\"Perl\",\"Ruby\"],{'hair':'black','eye':'blue'})]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField('name', StructType([\n",
    "            StructField('fname', StringType(), True),\n",
    "            StructField('lname', StringType(), True)])),\n",
    "        StructField('languages', ArrayType(StringType()),True),\n",
    "        StructField('properties', MapType(StringType(),StringType()),True)\n",
    "     ])\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|properties[hair]|\n",
      "+----------------+\n",
      "|           black|\n",
      "|           brown|\n",
      "|             red|\n",
      "|           black|\n",
      "+----------------+\n",
      "\n",
      "+----------+\n",
      "|name.fname|\n",
      "+----------+\n",
      "|     James|\n",
      "|       Ann|\n",
      "|Tom Cruise|\n",
      "| Tom Brand|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#getField from MapType\n",
    "df.select(df.properties.getField(\"hair\")).show()\n",
    "\n",
    "#getField from Struct\n",
    "df.select(df.name.getField(\"fname\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.14 getItem() – To get the value by index from MapType or ArrayTupe & ny key for MapType column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|languages[1]|\n",
      "+------------+\n",
      "|          C#|\n",
      "|      Python|\n",
      "|       Scala|\n",
      "|        Ruby|\n",
      "+------------+\n",
      "\n",
      "+----------------+\n",
      "|properties[hair]|\n",
      "+----------------+\n",
      "|           black|\n",
      "|           brown|\n",
      "|             red|\n",
      "|           black|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#getItem() used with ArrayType\n",
    "df.select(df.languages.getItem(1)).show()\n",
    "\n",
    "#getItem() used with MapType\n",
    "df.select(df.properties.getItem(\"hair\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0012cd877e37c553ce082c8a53dbb8150686811a71a997c633aba52086b562f8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pyspark3': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
