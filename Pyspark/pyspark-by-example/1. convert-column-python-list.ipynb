{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  We can get values from a pyspark columns using below way:\n",
    "\n",
    "    1. Using df.rdd.collect() or df.collect() and looping for each element in the list containing pyspark.sql.types.Row element\n",
    "    2. Using df.rdd.map(lambda x: x[3]).collect(). Here we would get list of original data type as the values.\n",
    "    3. Using df.select().collect(). This is same as first step only diff is we are extracting specific columns.\n",
    "    4. Using Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),(\"Michael\",\"Rose\",\"USA\",\"NY\"), \\\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),(\"Maria\",\"Jones\",\"USA\",\"FL\") \\\n",
    "  ]\n",
    "\n",
    "schemas=[\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "\n",
    "df=spark.createDataFrame(data=data,schema=schemas)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapPartitionsRDD[38] at javaToPython at NativeMethodAccessorImpl.java:0\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "rdd1 = df.rdd\n",
    "print(rdd1)\n",
    "print(type(rdd1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(firstname='James', lastname='Smith', country='USA', state='CA'),\n",
       " Row(firstname='Michael', lastname='Rose', country='USA', state='NY'),\n",
       " Row(firstname='Robert', lastname='Williams', country='USA', state='CA'),\n",
       " Row(firstname='Maria', lastname='Jones', country='USA', state='FL')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Using toDF() we can again convert RDD to Dataframe\n",
    "\n",
    "df1 = df.rdd.toDF()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To Get the data from RDD we use take(), collect()\n",
    "2. Collect after RDD makes it list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(firstname='James', lastname='Smith', country='USA', state='CA'), Row(firstname='Michael', lastname='Rose', country='USA', state='NY'), Row(firstname='Robert', lastname='Williams', country='USA', state='CA'), Row(firstname='Maria', lastname='Jones', country='USA', state='FL')]\n",
      "[Row(firstname='James', lastname='Smith', country='USA', state='CA'), Row(firstname='Michael', lastname='Rose', country='USA', state='NY'), Row(firstname='Robert', lastname='Williams', country='USA', state='CA'), Row(firstname='Maria', lastname='Jones', country='USA', state='FL')]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "## When we use collect() after rdd it become List\n",
    "print(df.collect())\n",
    "print(df.rdd.collect())\n",
    "print(type(df.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(firstname='James', lastname='Smith', country='USA', state='CA')\n",
      "<class 'pyspark.sql.types.Row'>\n",
      "firstname: James ,lastname: Smith , country: USA, state: CA\n",
      "Row(firstname='Michael', lastname='Rose', country='USA', state='NY')\n",
      "<class 'pyspark.sql.types.Row'>\n",
      "firstname: Michael ,lastname: Rose , country: USA, state: NY\n",
      "Row(firstname='Robert', lastname='Williams', country='USA', state='CA')\n",
      "<class 'pyspark.sql.types.Row'>\n",
      "firstname: Robert ,lastname: Williams , country: USA, state: CA\n",
      "Row(firstname='Maria', lastname='Jones', country='USA', state='FL')\n",
      "<class 'pyspark.sql.types.Row'>\n",
      "firstname: Maria ,lastname: Jones , country: USA, state: FL\n"
     ]
    }
   ],
   "source": [
    "## Get Value from Dataframe using RDD\n",
    "for elem in df.collect():\n",
    "    print(elem)\n",
    "    print(type(elem))\n",
    "    print(f\"firstname: {elem[0]} ,lastname: {elem[1]} , country: {elem[2]}, state: {elem[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(firstname='James', lastname='Smith', country='USA', state='CA')\n",
      "Row(firstname='Michael', lastname='Rose', country='USA', state='NY')\n",
      "Row(firstname='Robert', lastname='Williams', country='USA', state='CA')\n",
      "Row(firstname='Maria', lastname='Jones', country='USA', state='FL')\n",
      "Smith\n"
     ]
    }
   ],
   "source": [
    "print(df.collect()[0])\n",
    "print(df.collect()[1])\n",
    "print(df.collect()[2])\n",
    "print(df.collect()[3])\n",
    "print(df.collect()[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James\n",
      "Smith\n",
      "USA\n",
      "CA\n",
      "Michael\n",
      "Rose\n",
      "USA\n",
      "NY\n",
      "Robert\n",
      "Williams\n",
      "USA\n",
      "CA\n",
      "Maria\n",
      "Jones\n",
      "USA\n",
      "FL\n"
     ]
    }
   ],
   "source": [
    "## Another way to get the data from Dataframe using RDD\n",
    "\n",
    "for i in range(len(df.collect())):\n",
    "    for j in range(len(df.collect()[i])):\n",
    "        print(df.collect()[i][j])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CA', 'NY', 'CA', 'FL']\n",
      "['CA', 'NY', 'CA', 'FL']\n",
      "CA\n",
      "NY\n",
      "CA\n",
      "FL\n"
     ]
    }
   ],
   "source": [
    "## Using Lambda get data from RDD. This way we can get data from spark dataframe column\n",
    "\n",
    "states_list = df.rdd.map(lambda x: x[3]).collect()\n",
    "print(states_list)\n",
    "states_list = df.rdd.map(lambda x: x[\"state\"]).collect() ## This generate list containing str or int data\n",
    "print(states_list)\n",
    "\n",
    "for elem in states_list:\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import OrderedDict \n",
    "# res = list(OrderedDict.fromkeys(states_list)) \n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(state='CA'), Row(state='NY'), Row(state='CA'), Row(state='FL')]\n",
      "<class 'pyspark.sql.types.Row'>\n",
      "CA\n",
      "<class 'pyspark.sql.types.Row'>\n",
      "NY\n",
      "<class 'pyspark.sql.types.Row'>\n",
      "CA\n",
      "<class 'pyspark.sql.types.Row'>\n",
      "FL\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## ANother way to get data from dataframe column\n",
    "states3=df.select(df.state).collect()  ## This generate list containing row type data\n",
    "print(states3)\n",
    "for elem in states3:\n",
    "    print(type(elem))\n",
    "    print(elem[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So using map operation we can convert row type data to normal datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(state='CA', firstname='James'), Row(state='NY', firstname='Michael'), Row(state='CA', firstname='Robert'), Row(state='FL', firstname='Maria')]\n"
     ]
    }
   ],
   "source": [
    "states4=df.select(df.state,df.firstname).collect()  ## This generate list containing row type data\n",
    "print(states4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CA', 'NY', 'CA', 'FL']\n",
      "['James', 'Michael', 'Robert', 'Maria']\n"
     ]
    }
   ],
   "source": [
    "pandDF=df.select(df.state,df.firstname).toPandas()\n",
    "print(list(pandDF['state']))\n",
    "print(list(pandDF['firstname']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0012cd877e37c553ce082c8a53dbb8150686811a71a997c633aba52086b562f8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pyspark3': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
