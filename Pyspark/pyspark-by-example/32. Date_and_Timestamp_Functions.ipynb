{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark Date and Timestamp Functions are supported on DataFrame and SQL queries and they work similarly to traditional SQL, Date and Time are very important if you are using PySpark for ETL. Most of all these functions accept input as, Date type, Timestamp type, or String. If a String used, it should be in a default format that can be cast to date.\n",
    "\n",
    "    DateType default format is yyyy-MM-dd \n",
    "    TimestampType default format is yyyy-MM-dd HH:mm:ss.SSSS\n",
    "    Returns null if the input is a string that can not be cast to Date or Timestamp.\n",
    "    \n",
    "PySpark SQL provides several Date & Timestamp functions hence keep an eye on and understand these. Always you should choose these functions instead of writing your own functions (UDF) as these functions are compile-time safe, handles null, and perform better when compared to PySpark UDF. If your PySpark application is critical on performance try to avoid using custom UDF at all costs as these are not guarantee performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType,StructField, DateType, IntegerType,TimestampType,StringType\n",
    "schema = StructType([ \\\n",
    "    StructField(\"user_id\",IntegerType(),True), \\\n",
    "    StructField(\"movie_id\",IntegerType(),True), \\\n",
    "    StructField(\"rating\",IntegerType(),True), \\\n",
    "    StructField(\"date1\", StringType(), True)\n",
    "  ])\n",
    "uDataDF = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"false\").schema(schema).load(\"u.data.csv\",sep ='\\t')\n",
    "uDataDF = uDataDF.withColumn(\"date2\",from_unixtime(col(\"date1\")))\n",
    "uDataDF = uDataDF.withColumn(\"date\",date_format(col(\"date2\"), \"MM/dd/yyyy\")).orderBy(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- movie_id: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- date1: string (nullable = true)\n",
      " |-- date2: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uDataDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+---------+-------------------+----------+\n",
      "|user_id|movie_id|rating|    date1|              date2|      date|\n",
      "+-------+--------+------+---------+-------------------+----------+\n",
      "|     66|     181|     5|883601425|1998-01-01 02:20:25|01/01/1998|\n",
      "|      6|      69|     3|883601277|1998-01-01 02:17:57|01/01/1998|\n",
      "|      6|     357|     4|883602422|1998-01-01 02:37:02|01/01/1998|\n",
      "|      6|     517|     4|883602212|1998-01-01 02:33:32|01/01/1998|\n",
      "|     66|     298|     4|883601324|1998-01-01 02:18:44|01/01/1998|\n",
      "|      6|      86|     3|883603013|1998-01-01 02:46:53|01/01/1998|\n",
      "|     66|     258|     4|883601089|1998-01-01 02:14:49|01/01/1998|\n",
      "|     66|       1|     3|883601324|1998-01-01 02:18:44|01/01/1998|\n",
      "|      6|      98|     5|883600680|1998-01-01 02:08:00|01/01/1998|\n",
      "|      6|     492|     5|883601089|1998-01-01 02:14:49|01/01/1998|\n",
      "|     66|     877|     1|883601089|1998-01-01 02:14:49|01/01/1998|\n",
      "|     66|       7|     3|883601355|1998-01-01 02:19:15|01/01/1998|\n",
      "|      6|     463|     4|883601713|1998-01-01 02:25:13|01/01/1998|\n",
      "|      6|     478|     4|883602762|1998-01-01 02:42:42|01/01/1998|\n",
      "|     44|     294|     4|883612356|1998-01-01 05:22:36|01/01/1998|\n",
      "|      6|     508|     3|883599530|1998-01-01 01:48:50|01/01/1998|\n",
      "|      6|      23|     4|883601365|1998-01-01 02:19:25|01/01/1998|\n",
      "|      6|     469|     5|883601155|1998-01-01 02:15:55|01/01/1998|\n",
      "|     44|     449|     5|883613334|1998-01-01 05:38:54|01/01/1998|\n",
      "|     46|     307|     3|883611430|1998-01-01 05:07:10|01/01/1998|\n",
      "+-------+--------+------+---------+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uDataDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|     input|\n",
      "+---+----------+\n",
      "|  1|2020-02-01|\n",
      "|  2|2019-03-01|\n",
      "|  3|2021-03-01|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"]]\n",
    "df=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+\n",
      "| id|     input|current_date|\n",
      "+---+----------+------------+\n",
      "|  1|2020-02-01|  2022-01-21|\n",
      "|  2|2019-03-01|  2022-01-21|\n",
      "|  3|2021-03-01|  2022-01-21|\n",
      "+---+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df1 = df.withColumn(\"current_date\",current_date())\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- input: string (nullable = true)\n",
      " |-- current_date: date (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+\n",
      "| id|     input|date_format|\n",
      "+---+----------+-----------+\n",
      "|  1|2020-02-01| 02-01-2020|\n",
      "|  2|2019-03-01| 03-01-2019|\n",
      "|  3|2021-03-01| 03-01-2021|\n",
      "+---+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## date_format()\n",
    "df2 = df.select(col(\"*\"), \n",
    "    date_format(col(\"input\"), \"MM-dd-yyyy\").alias(\"date_format\") \n",
    "  )\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- input: string (nullable = true)\n",
      " |-- date_format: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to_date()\n",
    "\n",
    "Below example converts string in date format yyyy-MM-dd to a DateType yyyy-MM-dd using to_date(). You can also use this to convert into any specific format. PySpark supports all patterns supports on Java DateTimeFormatter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     input|   to_date|\n",
      "+----------+----------+\n",
      "|2020-02-01|2020-02-01|\n",
      "|2019-03-01|2019-03-01|\n",
      "|2021-03-01|2021-03-01|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 =df.select(col(\"input\"), \n",
    "    to_date(col(\"input\"), \"yyy-MM-dd\").alias(\"to_date\") \n",
    "  )\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- input: string (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datediff()\n",
    "df4 = df.select(col(\"input\"), \n",
    "    datediff(current_date(),col(\"input\")).alias(\"datediff\")  \n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|     input|datediff|\n",
      "+----------+--------+\n",
      "|2020-02-01|     721|\n",
      "|2019-03-01|    1058|\n",
      "|2021-03-01|     327|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- input: string (nullable = true)\n",
      " |-- datediff: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df3.select(col(\"to_date\"), \n",
    "    datediff(current_date(),col(\"to_date\")).alias(\"datediff\")  \n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|   to_date|datediff|\n",
      "+----------+--------+\n",
      "|2020-02-01|     721|\n",
      "|2019-03-01|    1058|\n",
      "|2021-03-01|     327|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- to_date: date (nullable = true)\n",
      " |-- datediff: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|   to_date|months_between|\n",
      "+----------+--------------+\n",
      "|2020-02-01|   23.67741935|\n",
      "|2019-03-01|   34.67741935|\n",
      "|2021-03-01|   10.67741935|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# months_between()\n",
    "# The below example returns the months between two dates using months_between().\n",
    "\n",
    "df6 = df3.select(col(\"to_date\"), \n",
    "    months_between(current_date(),col(\"to_date\")).alias(\"months_between\")  \n",
    "  )\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- to_date: date (nullable = true)\n",
      " |-- months_between: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+----------+\n",
      "|     input|add_months|sub_months|  date_add|  date_sub|  date_sub|\n",
      "+----------+----------+----------+----------+----------+----------+\n",
      "|2020-02-01|2020-05-01|2019-11-01|2020-02-05|2020-01-28|2020-01-28|\n",
      "|2019-03-01|2019-06-01|2018-12-01|2019-03-05|2019-02-25|2019-02-25|\n",
      "|2021-03-01|2021-06-01|2020-12-01|2021-03-05|2021-02-25|2021-02-25|\n",
      "+----------+----------+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add_months() , date_add(), date_sub()\n",
    "# Here we are adding and subtracting date and month from a given input.\n",
    "\n",
    "df.select(col(\"input\"), \n",
    "    add_months(col(\"input\"),3).alias(\"add_months\"), \n",
    "    add_months(col(\"input\"),-3).alias(\"sub_months\"), \n",
    "    date_add(col(\"input\"),4).alias(\"date_add\"), \n",
    "    date_add(col(\"input\"),-4).alias(\"date_sub\"),\n",
    "    date_sub(col(\"input\"),4).alias(\"date_sub\")\n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = df.select(col(\"input\"), \n",
    "    add_months(col(\"input\"),3).alias(\"add_months\"), \n",
    "    add_months(col(\"input\"),-3).alias(\"sub_months\"), \n",
    "    date_add(col(\"input\"),4).alias(\"date_add\"), \n",
    "    date_add(col(\"input\"),-4).alias(\"date_sub\"),\n",
    "    date_sub(col(\"input\"),4).alias(\"date_sub\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- input: string (nullable = true)\n",
      " |-- add_months: date (nullable = true)\n",
      " |-- sub_months: date (nullable = true)\n",
      " |-- date_add: date (nullable = true)\n",
      " |-- date_sub: date (nullable = true)\n",
      " |-- date_sub: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+----------+----------+\n",
      "|     input|year|month|  next_day|weekofyear|\n",
      "+----------+----+-----+----------+----------+\n",
      "|2020-02-01|2020|    2|2020-02-02|         5|\n",
      "|2019-03-01|2019|    3|2019-03-03|         9|\n",
      "|2021-03-01|2021|    3|2021-03-07|         9|\n",
      "+----------+----+-----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year(), month(), month(),next_day(), weekofyear()\n",
    "\n",
    "df8 = df.select(col(\"input\"), \n",
    "     year(col(\"input\")).alias(\"year\"), \n",
    "     month(col(\"input\")).alias(\"month\"), \n",
    "     next_day(col(\"input\"),\"Sunday\").alias(\"next_day\"), \n",
    "     weekofyear(col(\"input\")).alias(\"weekofyear\") \n",
    "  )\n",
    "df8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- input: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- next_day: date (nullable = true)\n",
      " |-- weekofyear: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+---------+\n",
      "|     input|dayofweek|dayofmonth|dayofyear|\n",
      "+----------+---------+----------+---------+\n",
      "|2020-02-01|        7|         1|       32|\n",
      "|2019-03-01|        6|         1|       60|\n",
      "|2021-03-01|        2|         1|       60|\n",
      "+----------+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dayofweek(), dayofmonth(), dayofyear()\n",
    "df.select(col(\"input\"),  \n",
    "     dayofweek(col(\"input\")).alias(\"dayofweek\"), \n",
    "     dayofmonth(col(\"input\")).alias(\"dayofmonth\"), \n",
    "     dayofyear(col(\"input\")).alias(\"dayofyear\"), \n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+\n",
      "|id |input                  |\n",
      "+---+-----------------------+\n",
      "|1  |02-01-2020 11 01 19 06 |\n",
      "|2  |03-01-2019 12 01 19 406|\n",
      "|3  |03-01-2021 12 01 19 406|\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# current_timestamp()\n",
    "# Following are the Timestamp Functions that you can use on SQL and on DataFrame. Let’s learn these with examples.\n",
    "\n",
    "data=[[\"1\",\"02-01-2020 11 01 19 06\"],[\"2\",\"03-01-2019 12 01 19 406\"],[\"3\",\"03-01-2021 12 01 19 406\"]]\n",
    "df2=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- input: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|current_timestamp      |\n",
      "+-----------------------+\n",
      "|2022-01-22 01:07:58.603|\n",
      "+-----------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df9 = df2.select(current_timestamp().alias(\"current_timestamp\")\n",
    "  )\n",
    "df9.show(1,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- current_timestamp: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df9.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------------+\n",
      "|input                  |to_timestamp           |\n",
      "+-----------------------+-----------------------+\n",
      "|02-01-2020 11 01 19 06 |2020-02-01 11:01:19.06 |\n",
      "|03-01-2019 12 01 19 406|2019-03-01 12:01:19.406|\n",
      "|03-01-2021 12 01 19 406|2021-03-01 12:01:19.406|\n",
      "+-----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to_timestamp()\n",
    "# Converts string timestamp to Timestamp type format.\n",
    "\n",
    "df2.select(col(\"input\"), \n",
    "    to_timestamp(col(\"input\"), \"MM-dd-yyyy HH mm ss SSS\").alias(\"to_timestamp\") \n",
    "  ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- input: string (nullable = true)\n",
      " |-- to_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(col(\"input\"), \n",
    "    to_timestamp(col(\"input\"), \"MM-dd-yyyy HH mm ss SSS\").alias(\"to_timestamp\") \n",
    "  ).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----+------+------+\n",
      "|input                  |hour|minute|second|\n",
      "+-----------------------+----+------+------+\n",
      "|2020-02-01 11:01:19.06 |11  |1     |19    |\n",
      "|2019-03-01 12:01:19.406|12  |1     |19    |\n",
      "|2021-03-01 12:01:19.406|12  |1     |19    |\n",
      "+-----------------------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hour(), Minute() and second()\n",
    "data=[[\"1\",\"2020-02-01 11:01:19.06\"],[\"2\",\"2019-03-01 12:01:19.406\"],[\"3\",\"2021-03-01 12:01:19.406\"]]\n",
    "df3=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "\n",
    "df3.select(col(\"input\"), \n",
    "    hour(col(\"input\")).alias(\"hour\"), \n",
    "    minute(col(\"input\")).alias(\"minute\"),\n",
    "    second(col(\"input\")).alias(\"second\") \n",
    "  ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://databricks.com/blog/2015/09/16/apache-spark-1-5-dataframe-api-highlights.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0012cd877e37c553ce082c8a53dbb8150686811a71a997c633aba52086b562f8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pyspark3': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
