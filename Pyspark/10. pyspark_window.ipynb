{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful Links:\n",
    "\n",
    "    https://medium.com/@sergey.ivanchuk/practical-pyspark-window-function-examples-cb5c7e1a3c41\n",
    "    https://www.educba.com/pyspark-window-functions/\n",
    "    https://knockdata.github.io/spark-window-function/\n",
    "    https://sparkbyexamples.com/pyspark/pyspark-window-functions/\n",
    "    https://www.geeksforgeeks.org/pyspark-window-functions/\n",
    "    https://towardsdatascience.com/spark-sql-102-aggregations-and-window-functions-9f829eaa7549\n",
    "    https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html\n",
    "    https://medium.com/analytics-vidhya/solving-complex-big-data-problems-using-combinations-of-window-functions-deep-dive-in-pyspark-b1830eb00b7d  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime,col, lit, udf, datediff, lead, explode,to_date\n",
    "from pyspark.sql import SparkSession,Window,DataFrame\n",
    "import datetime\n",
    "from pyspark.sql.types import StringType,BooleanType,DateType,LongType,ArrayType\n",
    "from typing import List\n",
    "import pyspark.sql.functions as F\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shopping_data = \\\n",
    "[('Alex','2018-10-10','Paint',80),('Alex','2018-04-02','Ladder',20),('Alex','2018-06-22','Stool',20),\\\n",
    "('Alex','2018-12-09','Vacuum',40),('Alex','2018-07-12','Bucket',5),('Alex','2018-02-18','Gloves',5),\\\n",
    "('Alex','2018-03-03','Brushes',30),('Alex','2018-09-26','Sandpaper',10)]\n",
    "\n",
    "df = spark.createDataFrame(shopping_data, ['name','date','product','price'])\\\n",
    "                .withColumn('date',F.col('date').cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, date: date, product: string, price: bigint]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(col(\"product\") == \"Paint\").orderBy(col(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+\n",
      "|name|      date|  product|price|\n",
      "+----+----------+---------+-----+\n",
      "|Alex|2018-10-10|    Paint|   80|\n",
      "|Alex|2018-04-02|   Ladder|   20|\n",
      "|Alex|2018-06-22|    Stool|   20|\n",
      "|Alex|2018-12-09|   Vacuum|   40|\n",
      "|Alex|2018-07-12|   Bucket|    5|\n",
      "|Alex|2018-02-18|   Gloves|    5|\n",
      "|Alex|2018-03-03|  Brushes|   30|\n",
      "|Alex|2018-09-26|Sandpaper|   10|\n",
      "+----+----------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+----------+\n",
      "|name|      date|  product|price|price_rank|\n",
      "+----+----------+---------+-----+----------+\n",
      "|Alex|2018-10-10|    Paint|   80|         1|\n",
      "|Alex|2018-12-09|   Vacuum|   40|         2|\n",
      "|Alex|2018-03-03|  Brushes|   30|         3|\n",
      "|Alex|2018-04-02|   Ladder|   20|         4|\n",
      "|Alex|2018-06-22|    Stool|   20|         4|\n",
      "|Alex|2018-09-26|Sandpaper|   10|         5|\n",
      "|Alex|2018-07-12|   Bucket|    5|         6|\n",
      "|Alex|2018-02-18|   Gloves|    5|         6|\n",
      "+----+----------+---------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w0 = Window.partitionBy('name')\n",
    "\n",
    "# Sort purchases by descending order of price and have continuous ranking for ties.\n",
    "\n",
    "df.withColumn(\"price_rank\",F.dense_rank().over(w0.orderBy(F.col('price').desc()))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+----------+\n",
      "|name|      date|  product|price|price_rank|\n",
      "+----+----------+---------+-----+----------+\n",
      "|Alex|2018-10-10|    Paint|   80|         1|\n",
      "|Alex|2018-12-09|   Vacuum|   40|         2|\n",
      "|Alex|2018-03-03|  Brushes|   30|         3|\n",
      "|Alex|2018-04-02|   Ladder|   20|         4|\n",
      "|Alex|2018-06-22|    Stool|   20|         4|\n",
      "|Alex|2018-09-26|Sandpaper|   10|         5|\n",
      "|Alex|2018-07-12|   Bucket|    5|         6|\n",
      "|Alex|2018-02-18|   Gloves|    5|         6|\n",
      "+----+----------+---------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1= Window.partitionBy('name').orderBy(F.col('price').desc())\n",
    "\n",
    "df.withColumn(\"price_rank\",F.dense_rank().over(w1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+----------+\n",
      "|name|      date|  product|price|price_rank|\n",
      "+----+----------+---------+-----+----------+\n",
      "|Alex|2018-07-12|   Bucket|    5|         1|\n",
      "|Alex|2018-02-18|   Gloves|    5|         1|\n",
      "|Alex|2018-09-26|Sandpaper|   10|         3|\n",
      "|Alex|2018-04-02|   Ladder|   20|         4|\n",
      "|Alex|2018-06-22|    Stool|   20|         4|\n",
      "|Alex|2018-03-03|  Brushes|   30|         6|\n",
      "|Alex|2018-12-09|   Vacuum|   40|         7|\n",
      "|Alex|2018-10-10|    Paint|   80|         8|\n",
      "+----+----------+---------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort purchases by ascending order of price and have skip rankings for ties.\n",
    "df.withColumn(\"price_rank\",F.rank().over(w0.orderBy(F.col('price').asc()))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+----------+\n",
      "|name|      date|  product|price|price_rank|\n",
      "+----+----------+---------+-----+----------+\n",
      "|Alex|2018-10-10|    Paint|   80|         1|\n",
      "|Alex|2018-12-09|   Vacuum|   40|         1|\n",
      "|Alex|2018-03-03|  Brushes|   30|         2|\n",
      "|Alex|2018-04-02|   Ladder|   20|         2|\n",
      "|Alex|2018-06-22|    Stool|   20|         3|\n",
      "|Alex|2018-09-26|Sandpaper|   10|         3|\n",
      "|Alex|2018-07-12|   Bucket|    5|         4|\n",
      "|Alex|2018-02-18|   Gloves|    5|         4|\n",
      "+----+----------+---------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bucket purchases into 4 tiles (e.g. least expensive, middle tiers and most expensive purchases). and sort descending order of price\n",
    "df.withColumn(\"price_rank\",F.ntile(4).over(w0.orderBy(F.col('price').desc()))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+-------------------+\n",
      "|name|      date|  product|price|     price_rel_rank|\n",
      "+----+----------+---------+-----+-------------------+\n",
      "|Alex|2018-10-10|    Paint|   80|                0.0|\n",
      "|Alex|2018-12-09|   Vacuum|   40|0.14285714285714285|\n",
      "|Alex|2018-03-03|  Brushes|   30| 0.2857142857142857|\n",
      "|Alex|2018-04-02|   Ladder|   20|0.42857142857142855|\n",
      "|Alex|2018-06-22|    Stool|   20|0.42857142857142855|\n",
      "|Alex|2018-09-26|Sandpaper|   10| 0.7142857142857143|\n",
      "|Alex|2018-07-12|   Bucket|    5| 0.8571428571428571|\n",
      "|Alex|2018-02-18|   Gloves|    5| 0.8571428571428571|\n",
      "+----+----------+---------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort purchases and generating a relative/percent rank to distance from max price.\n",
    "df.withColumn('price_rel_rank',F.percent_rank().over(w0.orderBy(F.col('price').desc()))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row Item Difference - Lead and Lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    The two functions below, lag and lead, are probably the most abstract examples in this article and could be confusing at first. \\n    The core concept here is essentially a subtraction between some row (e.g. current) and prior or future row(s). \\n    For examples, from the table below we can say “ 13 = (2018–03–03) — (2018–02–18) “ — which is a difference of days between two dates.\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    The two functions below, lag and lead, are probably the most abstract examples in this article and could be confusing at first. \n",
    "    The core concept here is essentially a subtraction between some row (e.g. current) and prior or future row(s). \n",
    "    For examples, from the table below we can say “ 13 = (2018–03–03) — (2018–02–18) “ — which is a difference of days between two dates.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+-----------------------+-------------------------+\n",
      "|name|      date|  product|price|days_from_last_purchase|days_before_next_purchase|\n",
      "+----+----------+---------+-----+-----------------------+-------------------------+\n",
      "|Alex|2018-02-18|   Gloves|    5|                   null|                       13|\n",
      "|Alex|2018-03-03|  Brushes|   30|                     13|                       30|\n",
      "|Alex|2018-04-02|   Ladder|   20|                     30|                       81|\n",
      "|Alex|2018-06-22|    Stool|   20|                     81|                       20|\n",
      "|Alex|2018-07-12|   Bucket|    5|                     20|                       76|\n",
      "|Alex|2018-09-26|Sandpaper|   10|                     76|                       14|\n",
      "|Alex|2018-10-10|    Paint|   80|                     14|                       60|\n",
      "|Alex|2018-12-09|   Vacuum|   40|                     60|                     null|\n",
      "+----+----------+---------+-----+-----------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('days_from_last_purchase', F.datediff('date',F.lag('date',1).over(w0.orderBy(F.col('date')))))\\\n",
    "  .withColumn('days_before_next_purchase', F.datediff(F.lead('date',1).over(w0.orderBy(F.col('date'))),'date'))\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This will give error as over not there\n",
    "# df.withColumn('days_from_last_purchase', F.datediff('date',F.lag('date',1)))\\\n",
    "#   .withColumn('days_before_next_purchase', F.datediff(F.lead('date',1),'date'))\\\n",
    "#   .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregations : Lists and Sets\n",
    "\n",
    "    Collect a set of prices ever paid (no duplicates) and collect a list of items paid at a certain price (permit duplicates).\n",
    "\n",
    "    I’m adding another purchase of paint to my data set in line 1 for the sake of example to generate duplicated items in lines 14 & 15 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+----------------+--------------------+\n",
      "|name|      date|  product|price|  items_by_price|          all_prices|\n",
      "+----+----------+---------+-----+----------------+--------------------+\n",
      "|Alex|2018-07-12|   Bucket|    5|[Bucket, Gloves]|[30, 5, 20, 10, 4...|\n",
      "|Alex|2018-02-18|   Gloves|    5|[Bucket, Gloves]|[30, 5, 20, 10, 4...|\n",
      "|Alex|2018-09-26|Sandpaper|   10|     [Sandpaper]|[30, 5, 20, 10, 4...|\n",
      "|Alex|2018-10-10|    Paint|   80|  [Paint, Paint]|[30, 5, 20, 10, 4...|\n",
      "|Alex|2018-10-11|    Paint|   80|  [Paint, Paint]|[30, 5, 20, 10, 4...|\n",
      "|Alex|2018-03-03|  Brushes|   30|       [Brushes]|[30, 5, 20, 10, 4...|\n",
      "|Alex|2018-04-02|   Ladder|   20| [Ladder, Stool]|[30, 5, 20, 10, 4...|\n",
      "|Alex|2018-06-22|    Stool|   20| [Ladder, Stool]|[30, 5, 20, 10, 4...|\n",
      "|Alex|2018-12-09|   Vacuum|   40|        [Vacuum]|[30, 5, 20, 10, 4...|\n",
      "+----+----------+---------+-----+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newRow = spark.createDataFrame([('Alex','2018-10-11','Paint',80)])\n",
    "df2 = df.union(newRow)\n",
    "\n",
    "df2.withColumn('items_by_price', F.collect_list('product').over(w0.partitionBy('price')))\\\n",
    "   .withColumn('all_prices',     F.collect_set('price').over(w0)).show()\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------------+\n",
      "|name|Price|           items|\n",
      "+----+-----+----------------+\n",
      "|Alex|    5|[Bucket, Gloves]|\n",
      "|Alex|   10|     [Sandpaper]|\n",
      "|Alex|   80|         [Paint]|\n",
      "|Alex|   30|       [Brushes]|\n",
      "|Alex|   20| [Ladder, Stool]|\n",
      "|Alex|   40|        [Vacuum]|\n",
      "+----+-----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.withColumn('items', F.collect_set('product').over(w0.partitionBy('price')))\\\n",
    "   .select('name','Price','items')\\\n",
    "   .distinct()\\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average, Sum, Max, Max within Rows, Counts\n",
    "    \n",
    "    Below are 5 very common calculations in single operation: avg + round, sum, max, max + rowsBetween ,\n",
    "    and count. They help us understanding various purchasing behavior about a profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+-----------+----------------+-----------+------------+-------------+\n",
      "|name|      date|  product|price|avg_to_date|accumulating_sum|max_to_date|max_of_last2|items_to_date|\n",
      "+----+----------+---------+-----+-----------+----------------+-----------+------------+-------------+\n",
      "|Alex|2018-02-18|   Gloves|    5|        5.0|               5|          5|           5|            1|\n",
      "|Alex|2018-03-03|  Brushes|   30|       17.5|              35|         30|          30|            2|\n",
      "|Alex|2018-04-02|   Ladder|   20|      18.33|              55|         30|          30|            3|\n",
      "|Alex|2018-06-22|    Stool|   20|      18.75|              75|         30|          20|            4|\n",
      "|Alex|2018-07-12|   Bucket|    5|       16.0|              80|         30|          20|            5|\n",
      "|Alex|2018-09-26|Sandpaper|   10|       15.0|              90|         30|          10|            6|\n",
      "|Alex|2018-10-10|    Paint|   80|      24.29|             170|         80|          80|            7|\n",
      "|Alex|2018-12-09|   Vacuum|   40|      26.25|             210|         80|          80|            8|\n",
      "+----+----------+---------+-----+-----------+----------------+-----------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1 = w0.orderBy(F.col('date'))\n",
    "\n",
    "df.withColumn('avg_to_date',     F.round(F.avg('price').over(w1),2))\\\n",
    "  .withColumn('accumulating_sum',F.sum('price').over(w1))\\\n",
    "  .withColumn('max_to_date',     F.max('price').over(w1))\\\n",
    "  .withColumn('max_of_last2',    F.max('price').over(w1.rowsBetween(-1,Window.currentRow)))\\\n",
    "  .withColumn('items_to_date',   F.count('*').over(w1))\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Complex Window Functions Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+------+------+------+\n",
      "|function_name|param1|param2|param3|result|\n",
      "+-------------+------+------+------+------+\n",
      "|           f1|     a|     b|     c|     1|\n",
      "|           f1|     b|     d|     m|     0|\n",
      "|           f2|     a|     b|     c|     0|\n",
      "|           f2|     b|     d|     m|     0|\n",
      "|           f3|     a|     b|     c|     1|\n",
      "|           f3|     b|     d|     m|     1|\n",
      "|           f4|     a|     b|     c|     0|\n",
      "|           f4|     b|     d|     m|     0|\n",
      "+-------------+------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list=[['f1','a','b','c',1],\n",
    "     ['f1','b','d','m',0],\n",
    "     ['f2','a','b','c',0],\n",
    "     ['f2','b','d','m',0],\n",
    "     ['f3','a','b','c',1],\n",
    "     ['f3','b','d','m',1],\n",
    "     ['f4','a','b','c',0],\n",
    "      ['f4','b','d','m',0]]\n",
    "\n",
    "df= spark.createDataFrame(list,['function_name','param1','param2','param3','result'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Required:\n",
    "\n",
    "result_list |  function_name_lists\n",
    "------------------------------------\n",
    "    [1,0]   |   [f1]\n",
    "    [0,0]   |   [f2,f4]\n",
    "    [1,1]   |   [f3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=Window().partitionBy(\"function_name\").orderBy(F.col(\"param1\"),F.col(\"param2\"),F.col(\"param3\"))\n",
    "w1 = Window.partitionBy(\"function_name\")\n",
    "\n",
    "df1 = df.withColumn(\"result_list\",F.collect_list(\"result\").over(w)).withColumn(\"rowNumber\",F.row_number().over(w)).withColumn(\"result3\",F.max(\"rowNumber\").over(w1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+------+------+------+-----------+---------+-------+\n",
      "|function_name|param1|param2|param3|result|result_list|rowNumber|result3|\n",
      "+-------------+------+------+------+------+-----------+---------+-------+\n",
      "|           f2|     a|     b|     c|     0|        [0]|        1|      2|\n",
      "|           f2|     b|     d|     m|     0|     [0, 0]|        2|      2|\n",
      "|           f4|     a|     b|     c|     0|        [0]|        1|      2|\n",
      "|           f4|     b|     d|     m|     0|     [0, 0]|        2|      2|\n",
      "|           f1|     a|     b|     c|     1|        [1]|        1|      2|\n",
      "|           f1|     b|     d|     m|     0|     [1, 0]|        2|      2|\n",
      "|           f3|     a|     b|     c|     1|        [1]|        1|      2|\n",
      "|           f3|     b|     d|     m|     1|     [1, 1]|        2|      2|\n",
      "+-------------+------+------+------+------+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|function_name|result_list|\n",
      "+-------------+-----------+\n",
      "|           f2|     [0, 0]|\n",
      "|           f4|     [0, 0]|\n",
      "|           f1|     [1, 0]|\n",
      "|           f3|     [1, 1]|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.filter(col(\"rowNumber\") == col(\"result3\")).drop(\"param1\",\"param2\",\"param3\",\"result\",\"rowNumber\",\"result3\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.groupBy(\"result_list\").agg(F.collect_list(\"function_name\").alias(\"function_name_list\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|result_list|function_name_list|\n",
      "+-----------+------------------+\n",
      "|     [1, 0]|              [f1]|\n",
      "|     [1, 1]|              [f3]|\n",
      "|     [0, 0]|          [f2, f4]|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input:\n",
    "![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/aec861e5-6496-4a0d-878a-0deeff55a0db/Untitled.png)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output\n",
    "\n",
    "\n",
    "![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/77514104-dccc-4116-99d4-f47db0cd947a/Untitled.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+---------+-----+\n",
      "|  item|store|timestamp|sales_qty|stock|\n",
      "+------+-----+---------+---------+-----+\n",
      "|673895|35578| 20180101|        1| null|\n",
      "|673895|35578| 20180102|        0|  110|\n",
      "|673895|35578| 20180103|        1| null|\n",
      "|673895|35578| 20180104|        7| null|\n",
      "|673895|35578| 20180105|        0|  112|\n",
      "|673895|35578| 20180106|        2| null|\n",
      "|673895|35578| 20180107|        0|  107|\n",
      "|673895|35578| 20180108|        0| null|\n",
      "|673895|35578| 20180109|        0| null|\n",
      "|673895|35578| 20180110|        1| null|\n",
      "+------+-----+---------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item =[[673895,35578,'20180101',1,None],\n",
    "       [673895,35578,'20180102',0,110],\n",
    "       [673895,35578,'20180103',1,None],\n",
    "       [673895,35578,'20180104',7,None],\n",
    "       [673895,35578,'20180105',0,112],\n",
    "       [673895,35578,'20180106',2,None],\n",
    "       [673895,35578,'20180107',0,107],\n",
    "       [673895,35578,'20180108',0,None],\n",
    "       [673895,35578,'20180109',0,None],\n",
    "       [673895,35578,'20180110',1,None]]\n",
    "\n",
    "df= spark.createDataFrame(item,['item','store','timestamp','sales_qty','stock'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item: long (nullable = true)\n",
      " |-- store: long (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- sales_qty: long (nullable = true)\n",
      " |-- stock: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+---------+-----+------+\n",
      "|  item|store|timestamp|sales_qty|stock|stock1|\n",
      "+------+-----+---------+---------+-----+------+\n",
      "|673895|35578| 20180101|        1| null|     0|\n",
      "|673895|35578| 20180102|        0|  110|   110|\n",
      "|673895|35578| 20180103|        1| null|     0|\n",
      "|673895|35578| 20180104|        7| null|     0|\n",
      "|673895|35578| 20180105|        0|  112|   112|\n",
      "|673895|35578| 20180106|        2| null|     0|\n",
      "|673895|35578| 20180107|        0|  107|   107|\n",
      "|673895|35578| 20180108|        0| null|     0|\n",
      "|673895|35578| 20180109|        0| null|     0|\n",
      "|673895|35578| 20180110|        1| null|     0|\n",
      "+------+-----+---------+---------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "w=Window().partitionBy(\"item\",\"store\").orderBy(\"timestamp\")\n",
    "df = df.withColumn(\"stock1\",when(col(\"stock\").isNull(),lit(0)).otherwise(col(\"stock\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+---------+-----+------+------+\n",
      "|  item|store|timestamp|sales_qty|stock|stock1|stock4|\n",
      "+------+-----+---------+---------+-----+------+------+\n",
      "|673895|35578| 20180101|        1| null|     0|     0|\n",
      "|673895|35578| 20180102|        0|  110|   110|     2|\n",
      "|673895|35578| 20180103|        1| null|     0|     0|\n",
      "|673895|35578| 20180104|        7| null|     0|     0|\n",
      "|673895|35578| 20180105|        0|  112|   112|     5|\n",
      "|673895|35578| 20180106|        2| null|     0|     0|\n",
      "|673895|35578| 20180107|        0|  107|   107|     7|\n",
      "|673895|35578| 20180108|        0| null|     0|     0|\n",
      "|673895|35578| 20180109|        0| null|     0|     0|\n",
      "|673895|35578| 20180110|        1| null|     0|     0|\n",
      "+------+-----+---------+---------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"stock4\", F.when(F.col(\"stock1\")!=0, F.rank().over(w)).otherwise(F.col(\"stock1\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+---------+-----+------+------+------+------+\n",
      "|  item|store|timestamp|sales_qty|stock|stock1|stock4|stock5|stock6|\n",
      "+------+-----+---------+---------+-----+------+------+------+------+\n",
      "|673895|35578| 20180101|        1| null|     0|     0|     0|     0|\n",
      "|673895|35578| 20180102|        0|  110|   110|     2|     2|   110|\n",
      "|673895|35578| 20180103|        1| null|     0|     0|     2|   110|\n",
      "|673895|35578| 20180104|        7| null|     0|     0|     2|   110|\n",
      "|673895|35578| 20180105|        0|  112|   112|     5|     7|   112|\n",
      "|673895|35578| 20180106|        2| null|     0|     0|     7|   112|\n",
      "|673895|35578| 20180107|        0|  107|   107|     7|    14|   107|\n",
      "|673895|35578| 20180108|        0| null|     0|     0|    14|   107|\n",
      "|673895|35578| 20180109|        0| null|     0|     0|    14|   107|\n",
      "|673895|35578| 20180110|        1| null|     0|     0|    14|   107|\n",
      "+------+-----+---------+---------+-----+------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w3=Window().partitionBy(\"item\",\"store\",\"stock5\").orderBy(\"timestamp\")\n",
    "df = df.withColumn(\"stock5\", F.sum(\"stock4\").over(w))\\\n",
    ".withColumn(\"stock6\", F.sum(\"stock1\").over(w3))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+---------+-----+------+------+------+------+---+------+\n",
      "|  item|store|timestamp|sales_qty|stock|stock1|stock4|stock5|stock6|sum|stock2|\n",
      "+------+-----+---------+---------+-----+------+------+------+------+---+------+\n",
      "|673895|35578| 20180101|        1|    0|     0|     0|     0|     0|  0|     0|\n",
      "|673895|35578| 20180102|        0|  110|   110|     2|     2|   110|  0|   110|\n",
      "|673895|35578| 20180103|        1|  109|     0|     0|     2|   110|  1|   109|\n",
      "|673895|35578| 20180104|        7|  102|     0|     0|     2|   110|  8|   102|\n",
      "|673895|35578| 20180105|        0|  112|   112|     5|     7|   112|  0|   112|\n",
      "|673895|35578| 20180106|        2|  110|     0|     0|     7|   112|  2|   110|\n",
      "|673895|35578| 20180107|        0|  107|   107|     7|    14|   107|  0|   107|\n",
      "|673895|35578| 20180108|        0|  107|     0|     0|    14|   107|  0|  null|\n",
      "|673895|35578| 20180109|        0|  107|     0|     0|    14|   107|  0|  null|\n",
      "|673895|35578| 20180110|        1|  106|     0|     0|    14|   107|  1|   106|\n",
      "+------+-----+---------+---------+-----+------+------+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"sum\", F.sum(F.when(F.col(\"stock1\")!=F.col(\"stock6\"),F.col(\"sales_qty\")).otherwise(F.lit(0))).over(w3))\\\n",
    ".withColumn(\"stock2\", F.when(F.col(\"sales_qty\")!=0, F.col(\"stock6\")-F.col(\"sum\")).otherwise(F.col(\"stock\")))\\\n",
    ".withColumn(\"stock\", F.when((F.col(\"stock2\").isNull())&(F.col(\"sales_qty\")==0),F.col(\"stock6\")-F.col(\"sum\")).otherwise(F.col(\"stock2\")))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+---------+-----+\n",
      "|  item|store|timestamp|sales_qty|stock|\n",
      "+------+-----+---------+---------+-----+\n",
      "|673895|35578| 20180101|        1|    0|\n",
      "|673895|35578| 20180102|        0|  110|\n",
      "|673895|35578| 20180103|        1|  109|\n",
      "|673895|35578| 20180104|        7|  102|\n",
      "|673895|35578| 20180105|        0|  112|\n",
      "|673895|35578| 20180106|        2|  110|\n",
      "|673895|35578| 20180107|        0|  107|\n",
      "|673895|35578| 20180108|        0|  107|\n",
      "|673895|35578| 20180109|        0|  107|\n",
      "|673895|35578| 20180110|        1|  106|\n",
      "+------+-----+---------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop(\"stock1\",\"stock4\",\"stock5\",\"stock6\",\"sum\",\"stock2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: Given a time series data which is a clickstream of user activity is stored in hive, ask is to enrich the data with session id using spark.\n",
    "\n",
    "Session Definition\n",
    "\n",
    "    Session expires after inactivity of 1 hour\n",
    "    Session remains active for a total duration of 2 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime,col, lit, udf, datediff, lead, explode,to_date\n",
    "from pyspark.sql import SparkSession,Window,DataFrame\n",
    "import datetime\n",
    "from pyspark.sql.types import StringType,BooleanType,DateType,LongType,ArrayType\n",
    "from typing import List\n",
    "import pyspark.sql.functions as f\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "streaming_data=[(\"U1\",\"2019-01-01T11:00:00Z\") , \n",
    "(\"U1\",\"2019-01-01T11:15:00Z\") , \n",
    "(\"U1\",\"2019-01-01T12:00:00Z\") , \n",
    "(\"U1\",\"2019-01-01T12:20:00Z\") , \n",
    "(\"U1\",\"2019-01-01T15:00:00Z\") , \n",
    "(\"U2\",\"2019-01-01T11:00:00Z\") , \n",
    "(\"U2\",\"2019-01-02T11:00:00Z\") , \n",
    "(\"U2\",\"2019-01-02T11:25:00Z\") , \n",
    "(\"U2\",\"2019-01-02T11:50:00Z\") , \n",
    "(\"U2\",\"2019-01-02T12:15:00Z\") , \n",
    "(\"U2\",\"2019-01-02T12:40:00Z\") , \n",
    "(\"U2\",\"2019-01-02T13:05:00Z\") , \n",
    "(\"U2\",\"2019-01-02T13:20:00Z\") ]\n",
    "schema=(\"UserId\",\"Click_Time\")\n",
    "df_stream=spark.createDataFrame(streaming_data,schema)\n",
    "df_stream=df_stream.withColumn(\"Click_Time\",df_stream[\"Click_Time\"].cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+\n",
      "|UserId|         Click_Time|\n",
      "+------+-------------------+\n",
      "|    U1|2019-01-01 16:30:00|\n",
      "|    U1|2019-01-01 16:45:00|\n",
      "|    U1|2019-01-01 17:30:00|\n",
      "|    U1|2019-01-01 17:50:00|\n",
      "|    U1|2019-01-01 20:30:00|\n",
      "|    U2|2019-01-01 16:30:00|\n",
      "|    U2|2019-01-02 16:30:00|\n",
      "|    U2|2019-01-02 16:55:00|\n",
      "|    U2|2019-01-02 17:20:00|\n",
      "|    U2|2019-01-02 17:45:00|\n",
      "|    U2|2019-01-02 18:10:00|\n",
      "|    U2|2019-01-02 18:35:00|\n",
      "|    U2|2019-01-02 18:50:00|\n",
      "+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stream.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+------------------+\n",
      "|UserId|         Click_Time|         time_diff|\n",
      "+------+-------------------+------------------+\n",
      "|    U2|2019-01-01 16:30:00|               0.0|\n",
      "|    U2|2019-01-02 16:30:00|              24.0|\n",
      "|    U2|2019-01-02 16:55:00|0.4166666666666667|\n",
      "|    U2|2019-01-02 17:20:00|0.4166666666666667|\n",
      "|    U2|2019-01-02 17:45:00|0.4166666666666667|\n",
      "|    U2|2019-01-02 18:10:00|0.4166666666666667|\n",
      "|    U2|2019-01-02 18:35:00|0.4166666666666667|\n",
      "|    U2|2019-01-02 18:50:00|              0.25|\n",
      "|    U1|2019-01-01 16:30:00|               0.0|\n",
      "|    U1|2019-01-01 16:45:00|              0.25|\n",
      "|    U1|2019-01-01 17:30:00|              0.75|\n",
      "|    U1|2019-01-01 17:50:00|0.3333333333333333|\n",
      "|    U1|2019-01-01 20:30:00|2.6666666666666665|\n",
      "+------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec=Window.partitionBy(\"UserId\").orderBy(\"Click_Time\")\n",
    "df_stream=df_stream.withColumn(\"time_diff\",(f.unix_timestamp(\"Click_Time\")-f.unix_timestamp(f.lag(f.col(\"Click_Time\"),1).over(window_spec)))/(60*60)).na.fill(0)\n",
    "df_stream.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+------------------+-----+------------+\n",
      "|UserId|         Click_Time|         time_diff|cond_|temp_session|\n",
      "+------+-------------------+------------------+-----+------------+\n",
      "|    U2|2019-01-01 16:30:00|               0.0|    0|           0|\n",
      "|    U2|2019-01-02 16:30:00|              24.0|    1|           1|\n",
      "|    U2|2019-01-02 16:55:00|0.4166666666666667|    0|           1|\n",
      "|    U2|2019-01-02 17:20:00|0.4166666666666667|    0|           1|\n",
      "|    U2|2019-01-02 17:45:00|0.4166666666666667|    0|           1|\n",
      "|    U2|2019-01-02 18:10:00|0.4166666666666667|    0|           1|\n",
      "|    U2|2019-01-02 18:35:00|0.4166666666666667|    0|           1|\n",
      "|    U2|2019-01-02 18:50:00|              0.25|    0|           1|\n",
      "|    U1|2019-01-01 16:30:00|               0.0|    0|           0|\n",
      "|    U1|2019-01-01 16:45:00|              0.25|    0|           0|\n",
      "|    U1|2019-01-01 17:30:00|              0.75|    0|           0|\n",
      "|    U1|2019-01-01 17:50:00|0.3333333333333333|    0|           0|\n",
      "|    U1|2019-01-01 20:30:00|2.6666666666666665|    1|           1|\n",
      "+------+-------------------+------------------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stream=df_stream.withColumn(\"cond_\",f.when(f.col(\"time_diff\")>1,1).otherwise(0))\n",
    "df_stream=df_stream.withColumn(\"temp_session\",f.sum(f.col(\"cond_\")).over(window_spec))\n",
    "df_stream.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+------------------+-----+------------+-------------+\n",
      "|UserId|         Click_Time|         time_diff|cond_|temp_session|2hr_time_diff|\n",
      "+------+-------------------+------------------+-----+------------+-------------+\n",
      "|    U2|2019-01-01 16:30:00|               0.0|    0|           0|            0|\n",
      "|    U2|2019-01-02 16:30:00|              24.0|    1|           1|            0|\n",
      "|    U2|2019-01-02 16:55:00|0.4166666666666667|    0|           1|         1500|\n",
      "|    U2|2019-01-02 17:20:00|0.4166666666666667|    0|           1|         1500|\n",
      "|    U2|2019-01-02 17:45:00|0.4166666666666667|    0|           1|         1500|\n",
      "|    U2|2019-01-02 18:10:00|0.4166666666666667|    0|           1|         1500|\n",
      "|    U2|2019-01-02 18:35:00|0.4166666666666667|    0|           1|         1500|\n",
      "|    U2|2019-01-02 18:50:00|              0.25|    0|           1|          900|\n",
      "|    U1|2019-01-01 16:30:00|               0.0|    0|           0|            0|\n",
      "|    U1|2019-01-01 16:45:00|              0.25|    0|           0|          900|\n",
      "|    U1|2019-01-01 17:30:00|              0.75|    0|           0|         2700|\n",
      "|    U1|2019-01-01 17:50:00|0.3333333333333333|    0|           0|         1200|\n",
      "|    U1|2019-01-01 20:30:00|2.6666666666666665|    1|           1|            0|\n",
      "+------+-------------------+------------------+-----+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_window=Window.partitionBy(\"UserId\",\"temp_session\").orderBy(\"Click_Time\")\n",
    "cond_2hr=(f.unix_timestamp(\"Click_Time\")-f.unix_timestamp(f.lag(f.col(\"Click_Time\"),1).over(new_window)))\n",
    "df_stream=df_stream.withColumn(\"2hr_time_diff\", cond_2hr).na.fill(0)\n",
    "df_stream.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+------------------+-----+------------+-------------+----------------+\n",
      "|UserId|         Click_Time|         time_diff|cond_|temp_session|2hr_time_diff|temp_session_2hr|\n",
      "+------+-------------------+------------------+-----+------------+-------------+----------------+\n",
      "|    U2|2019-01-01 16:30:00|               0.0|    0|           0|            0|               0|\n",
      "|    U2|2019-01-02 16:30:00|              24.0|    1|           1|            0|               0|\n",
      "|    U2|2019-01-02 16:55:00|0.4166666666666667|    0|           1|         1500|               0|\n",
      "|    U2|2019-01-02 17:20:00|0.4166666666666667|    0|           1|         1500|               0|\n",
      "|    U2|2019-01-02 17:45:00|0.4166666666666667|    0|           1|         1500|               0|\n",
      "|    U2|2019-01-02 18:10:00|0.4166666666666667|    0|           1|         1500|               0|\n",
      "|    U2|2019-01-02 18:35:00|0.4166666666666667|    0|           1|         1500|               1|\n",
      "|    U2|2019-01-02 18:50:00|              0.25|    0|           1|          900|               1|\n",
      "|    U1|2019-01-01 16:30:00|               0.0|    0|           0|            0|               0|\n",
      "|    U1|2019-01-01 16:45:00|              0.25|    0|           0|          900|               0|\n",
      "|    U1|2019-01-01 17:30:00|              0.75|    0|           0|         2700|               0|\n",
      "|    U1|2019-01-01 17:50:00|0.3333333333333333|    0|           0|         1200|               0|\n",
      "|    U1|2019-01-01 20:30:00|2.6666666666666665|    1|           1|            0|               0|\n",
      "+------+-------------------+------------------+-----+------------+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_spec=new_window.rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "df_stream=df_stream.withColumn(\"temp_session_2hr\",f.when(f.sum(f.col(\"2hr_time_diff\")).over(new_spec)-(2*60*60)>0,1).otherwise(0))\n",
    "df_stream.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+------------------+-----+------------+-------------+----------------+--------------------+\n",
      "|UserId|         Click_Time|         time_diff|cond_|temp_session|2hr_time_diff|temp_session_2hr|final_session_groups|\n",
      "+------+-------------------+------------------+-----+------------+-------------+----------------+--------------------+\n",
      "|    U2|2019-01-01 16:30:00|               0.0|    0|           0|            0|               0|                   0|\n",
      "|    U2|2019-01-02 16:30:00|              24.0|    1|           1|            0|               0|                   0|\n",
      "|    U2|2019-01-02 16:55:00|0.4166666666666667|    0|           1|         1500|               0|                   0|\n",
      "|    U2|2019-01-02 17:20:00|0.4166666666666667|    0|           1|         1500|               0|                   0|\n",
      "|    U2|2019-01-02 17:45:00|0.4166666666666667|    0|           1|         1500|               0|                   0|\n",
      "|    U2|2019-01-02 18:10:00|0.4166666666666667|    0|           1|         1500|               0|                   0|\n",
      "|    U2|2019-01-02 18:35:00|0.4166666666666667|    0|           1|         1500|               1|                   0|\n",
      "|    U2|2019-01-02 18:50:00|              0.25|    0|           1|          900|               1|                   0|\n",
      "|    U1|2019-01-01 16:30:00|               0.0|    0|           0|            0|               0|                   0|\n",
      "|    U1|2019-01-01 16:45:00|              0.25|    0|           0|          900|               0|                   0|\n",
      "|    U1|2019-01-01 17:30:00|              0.75|    0|           0|         2700|               0|                   0|\n",
      "|    U1|2019-01-01 17:50:00|0.3333333333333333|    0|           0|         1200|               0|                   0|\n",
      "|    U1|2019-01-01 20:30:00|2.6666666666666665|    1|           1|            0|               0|                   0|\n",
      "+------+-------------------+------------------+-----+------------+-------------+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_window_2hr=Window.partitionBy([\"UserId\",\"temp_session\",\"temp_session_2hr\"]).orderBy(\"Click_Time\")\n",
    "hrs_cond_=(f.when(f.unix_timestamp(f.col(\"Click_Time\"))-f.unix_timestamp(f.first(f.col(\"Click_Time\")).over(new_window_2hr))-(2*60*60)>0,1).otherwise(0))\n",
    "df_stream=df_stream.withColumn(\"final_session_groups\",hrs_cond_)\n",
    "df_stream.show(20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-----+-------------+-------------+----------+\n",
      "|UserId|Click_Time         |cond_|2hr_time_diff|final_session|session_id|\n",
      "+------+-------------------+-----+-------------+-------------+----------+\n",
      "|U2    |2019-01-01 16:30:00|0    |0            |1            |U21       |\n",
      "|U2    |2019-01-02 16:30:00|1    |0            |2            |U22       |\n",
      "|U2    |2019-01-02 16:55:00|0    |1500         |2            |U22       |\n",
      "|U2    |2019-01-02 17:20:00|0    |1500         |2            |U22       |\n",
      "|U2    |2019-01-02 17:45:00|0    |1500         |2            |U22       |\n",
      "|U2    |2019-01-02 18:10:00|0    |1500         |2            |U22       |\n",
      "|U2    |2019-01-02 18:35:00|0    |1500         |3            |U23       |\n",
      "|U2    |2019-01-02 18:50:00|0    |900          |3            |U23       |\n",
      "|U1    |2019-01-01 16:30:00|0    |0            |1            |U11       |\n",
      "|U1    |2019-01-01 16:45:00|0    |900          |1            |U11       |\n",
      "|U1    |2019-01-01 17:30:00|0    |2700         |1            |U11       |\n",
      "|U1    |2019-01-01 17:50:00|0    |1200         |1            |U11       |\n",
      "|U1    |2019-01-01 20:30:00|1    |0            |2            |U12       |\n",
      "+------+-------------------+-----+-------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stream=df_stream.withColumn(\"final_session\",df_stream[\"temp_session_2hr\"]+df_stream[\"temp_session\"]+df_stream[\"final_session_groups\"]+1)\\\n",
    ".drop(\"temp_session\",\"final_session_groups\",\"time_diff\",\"temp_session_2hr\",\"final_session_groups\")\n",
    "df_stream=df_stream.withColumn(\"session_id\",f.concat(f.col(\"UserId\"),f.col(\"final_session\")))\n",
    "df_stream.show(20,0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0012cd877e37c553ce082c8a53dbb8150686811a71a997c633aba52086b562f8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pyspark3': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
