{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "repartition() is used to increase or decrease the RDD/DataFrame partitions whereas the PySpark \n",
    "\n",
    "coalesce() is used to only decrease the number of partitions in an efficient way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com') \\\n",
    "        .master(\"local[5]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(0,20)\n",
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From local[5] 5\n"
     ]
    }
   ],
   "source": [
    "#spark.conf.set(\"spark.sql.shuffle.partitions\", \"500\")\n",
    "rdd = spark.sparkContext.parallelize((0,20))\n",
    "print(\"From local[5] \"+str(rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallelize : 6\n"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.sparkContext.parallelize((0,25), 6)\n",
    "print(\"parallelize : \"+str(rdd1.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextFile : 11\n"
     ]
    }
   ],
   "source": [
    "rddFromFile = spark.sparkContext.textFile(\"data/data.txt\",10) ## Need to check why it is 11\n",
    "print(\"TextFile : \"+str(rddFromFile.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1.saveAsTextFile(\"/tmp/partition3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartition size : 4\n"
     ]
    }
   ],
   "source": [
    "rdd2 = rdd1.repartition(4)\n",
    "print(\"Repartition size : \"+str(rdd2.getNumPartitions()))\n",
    "rdd2.saveAsTextFile(\"c://tmp/re-partition3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartition size : 6\n"
     ]
    }
   ],
   "source": [
    "rdd3 = rdd1.coalesce(6)\n",
    "print(\"Repartition size : \"+str(rdd3.getNumPartitions()))\n",
    "rdd3.saveAsTextFile(\"c:/tmp/coalesce3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "df=spark.range(0,20)\n",
    "print(df.rdd.getNumPartitions())\n",
    "\n",
    "df.write.mode(\"overwrite\").csv(\"c:/tmp/partition.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "df2 = df.repartition(6)\n",
    "print(df2.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "df3 = df.coalesce(2)\n",
    "print(df3.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "df3 = df.coalesce(8)  ## Though we are specify 8 here we can see it is taking default 5\n",
    "print(df3.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "df4 = df.groupBy(\"id\").count()\n",
    "print(df4.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.option(\"header\",True) \\\n",
    "        .csv(\"data/simple-zipcodes.csv\")  ## As it is single csv file with no partition\n",
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "newDF=df.repartition(3)\n",
    "print(newDF.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF.write.option(\"header\",True).mode(\"overwrite\") \\\n",
    "        .csv(\"/tmp/zipcodes-state\")\n",
    "df2=df.repartition(3,\"state\")\n",
    "df2.write.option(\"header\",True).mode(\"overwrite\") \\\n",
    "   .csv(\"data/zipcodes-state-3states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df.repartition(\"state\")\n",
    "df3.write.option(\"header\",True).mode(\"overwrite\") \\\n",
    "   .csv(\"data/zipcodes-state-allstates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0012cd877e37c553ce082c8a53dbb8150686811a71a997c633aba52086b562f8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pyspark3': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
